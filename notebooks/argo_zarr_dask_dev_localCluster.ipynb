{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f7dccf4",
   "metadata": {},
   "source": [
    "### Python imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4b63fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from time import sleep\n",
    "from functools import partial\n",
    "import time\n",
    "import numcodecs\n",
    "import zarr\n",
    "import dask\n",
    "from dask.distributed import as_completed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4847f77",
   "metadata": {},
   "source": [
    "### Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f831b415",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import boto3\n",
    "def read_dataset_inmemory(s3_path: str) -> xr.Dataset:\n",
    "    \"\"\"Read a NetCDF as an XArray using in-memory data\"\"\"\n",
    "    try:\n",
    "        with io.BytesIO() as inmemoryfile:\n",
    "            # Use boto to download a file to memory\n",
    "            session = boto3.Session(profile_name='nonproduction-admin')\n",
    "            s3 = session.client('s3')\n",
    "            bucket, key = s3_path.replace(\"s3://\", \"\").split(\"/\", 1)\n",
    "            s3.download_fileobj(bucket, key, inmemoryfile)\n",
    "            inmemoryfile.seek(0)\n",
    "            return xr.open_dataset(inmemoryfile)\n",
    "    except ValueError as e:\n",
    "        print(f\"Failed to open the file with error: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92551d5a",
   "metadata": {},
   "source": [
    "### Argo Processor\n",
    "\n",
    "**Source notebook**\n",
    "https://medium.com/@nicolasmortimer/argo-floats-zarr-and-pangeo-d74fc6d4ce35\n",
    "\n",
    "*Written by Nicolas Mortimer*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831b9e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_types ={'CONFIG_MISSION_NUMBER':'float32','CYCLE_NUMBER':'float32','DATA_CENTRE':'|U2','DATA_MODE':'|U1',\n",
    "             'DATA_STATE_INDICATOR':'|U4','DC_REFERENCE':'|U32','DIRECTION':'|U1','FIRMWARE_VERSION':'|U32',\n",
    "             'FLOAT_SERIAL_NO':'|U32','JULD':'float32','JULD_LOCATION':'float32','JULD_QC':'|U1','LATITUDE':'float32',\n",
    "             'LONGITUDE':'float32','PI_NAME':'|U64','PLATFORM_NUMBER':'|U8','PLATFORM_TYPE':'|U32','POSITIONING_SYSTEM':'|U8',\n",
    "             'POSITION_QC':'|U1','PRES':'float32','PRES_ADJUSTED':'float32','PRES_ADJUSTED_ERROR':'float32',\n",
    "             'PRES_ADJUSTED_QC':'|U1','PRES_QC':'|U1','PROFILE_PRES_QC':'|U1','PROFILE_PSAL_QC':'|U1','PROFILE_TEMP_QC':'|U1',\n",
    "             'PROJECT_NAME':'|U64','PSAL':'float32','PSAL_ADJUSTED':'float32','PSAL_ADJUSTED_ERROR':'float32',\n",
    "             'PSAL_ADJUSTED_QC':'|U1','PSAL_QC':'|U1','TEMP':'float32','TEMP_ADJUSTED':'float32','TEMP_ADJUSTED_ERROR':'float32',\n",
    "             'TEMP_ADJUSTED_QC':'|U1','TEMP_QC':'|U1','VERTICAL_SAMPLING_SCHEME':'|U256','WMO_INST_TYPE':'|U4'}\n",
    "\n",
    "data_levels =['PRES','PRES_ADJUSTED','PRES_ADJUSTED_ERROR','PRES_ADJUSTED_QC','PRES_QC','PSAL','PSAL_ADJUSTED',\n",
    "              'PSAL_ADJUSTED_ERROR','PSAL_ADJUSTED_QC','PSAL_QC','TEMP','TEMP_ADJUSTED','TEMP_ADJUSTED_ERROR',\n",
    "              'TEMP_ADJUSTED_QC','TEMP_QC']\n",
    "\n",
    "def process_mf(dsinput,levels,data_types=data_types,data_levels=data_levels):\n",
    "    ds = xr.Dataset()\n",
    "    dims =('N_PROF','N_LEVELS')\n",
    "    # The number of profiles is indicated by the N_PROF dimension\n",
    "    # The number of pressure levels is indicated by the N_LEVELS dimension\n",
    "    pading =xr.DataArray(np.ones((len(dsinput.N_PROF),levels-len( dsinput.N_LEVELS))) *np.nan,dims=dims)\n",
    "    pad_qc = xr.DataArray(np.chararray((len(dsinput.N_PROF),levels-len( dsinput.N_LEVELS))),dims=dims)\n",
    "    pad_qc[:] = b' '\n",
    "    for varname in data_types.keys():\n",
    "        if varname in dsinput.data_vars:\n",
    "            da = dsinput[varname]\n",
    "            if 'N_LEVELS' in da.dims:   \n",
    "                if varname in dsinput.data_vars:\n",
    "                    if varname.endswith('QC'):\n",
    "                        da = xr.concat([dsinput[varname],pad_qc],dim='N_LEVELS').astype(data_types[varname])\n",
    "                    else:\n",
    "                        da = xr.concat([dsinput[varname],pading],dim='N_LEVELS').astype(data_types[varname])\n",
    "            else:\n",
    "                da = dsinput[varname].astype(data_types[varname])\n",
    "        else:\n",
    "            if varname in data_levels:\n",
    "                if data_types[varname]=='float32':\n",
    "                    da = xr.DataArray(np.ones((len(dsinput.N_PROF),levels), dtype='float32')*np.nan , name=varname, dims=['N_PROF','N_LEVELS'])\n",
    "                else:\n",
    "                    p=np.chararray((len(dsinput.N_PROF),levels))\n",
    "                    p[:]=b'0'\n",
    "                    da = xr.DataArray(p.astype(data_types[varname]), name=varname, dims=['N_PROF','N_LEVELS'])\n",
    "            else:\n",
    "                if data_types[varname]=='float32':\n",
    "                    da = xr.DataArray(np.ones(len(dsinput.N_PROF), dtype=\"float32\")*np.nan , name=varname, dims=['N_PROF'])\n",
    "                else:\n",
    "                    p=np.chararray((len(dsinput.N_PROF)))\n",
    "                    p[:]=b'0'\n",
    "                    da = xr.DataArray(p.astype(data_types[varname]), name=varname, dims=['N_PROF'])\n",
    "        if not ('HISTORY' in varname) and ('N_CALIB' not in da.dims) and ('N_PARAM' not in da.dims) and  ('N_PROF' in da.dims):\n",
    "                ds[varname]= da\n",
    "    return ds.chunk({'N_LEVELS':levels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf5c52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(lst, n):\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ef375e",
   "metadata": {},
   "source": [
    "### Generate dataset and export to Zarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c83189",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import s3fs\n",
    "source_path = 's3://imos-data/IMOS/Argo/dac/csiro/1901119/profiles/*.nc'\n",
    "store_path = 's3://imos-data-pixeldrill/vhnguyen/test/temp/temp.zarr'\n",
    "aws_profile = 'nonproduction-admin'\n",
    "s3 = s3fs.S3FileSystem(profile=aws_profile, anon=False)\n",
    "glob_result = s3.glob(source_path)\n",
    "input_paths = []\n",
    "input_paths.extend(['s3://' + path for path in glob_result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beef26c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_paths[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0911aa45",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_dataset_inmemory(input_paths[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278facc1",
   "metadata": {},
   "source": [
    "### Using Dask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2ab2bf",
   "metadata": {},
   "source": [
    "#### Fargate cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3925a6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dask_cloudprovider.aws import FargateCluster\n",
    "# cluster = FargateCluster(image=\"ghcr.io/vietnguyengit/vietnguyen-dask:main\", scheduler_timeout=\"60 minutes\", task_role_arn=\"arn:aws:iam::615645230945:role/ManualDaskZarrCreation\",\n",
    "#                          scheduler_cpu=4096, scheduler_mem=30720, n_workers=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d71ebd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dask.distributed import Client\n",
    "# client = Client(cluster)\n",
    "# display(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b99c55",
   "metadata": {},
   "source": [
    "#### Local cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0eaa7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "from dask.distributed import Client, LocalCluster\n",
    "\n",
    "dask_user = 'vietnguyen'\n",
    "dask_address = 'localhost'\n",
    "dask_port = '41055'\n",
    "\n",
    "with dask.config.set({'temporary_directory': f'/home/{dask_user}/dask/'}):\n",
    "    # set up cluster and workers\n",
    "    cluster = LocalCluster(n_workers=4, memory_limit='8GB', processes=True, \n",
    "                           threads_per_worker=4, dashboard_address=f':{dask_port}', ip=dask_address)\n",
    "    client = Client(address=cluster.scheduler_address)\n",
    "\n",
    "print(f'http://{dask_address}'+':{port}/status'.format(port=client.scheduler_info().get('services').get('dashboard')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371a7371",
   "metadata": {},
   "source": [
    "#### Dask extension local cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1525c5-c610-4bb3-a52a-01e81a4a044e",
   "metadata": {},
   "source": [
    "Paths and store declaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88db411-12e6-4196-9e50-e0208bfab11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_paths = input_paths[:8]\n",
    "all_chunked_paths = list(chunks(input_paths, 2))\n",
    "store= s3fs.S3Map(root=f'{store_path}', s3=s3, check=False)\n",
    "start_from = 0 # index of all_chunked_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11496686-e109-46d9-8026-f2062357c827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datetime import datetime\n",
    "# ds1 = xr.Dataset(data_vars={'val': 1}, coords={'time': datetime(2022, 1, 1)})\n",
    "# ds1 = ds1.expand_dims(dim='time')\n",
    "# ds2 = xr.Dataset(data_vars={'val': 2}, coords={'time': datetime(2022, 1, 2)})\n",
    "# ds2 = ds2.expand_dims(dim='time')\n",
    "# ds3 = xr.Dataset(data_vars={'val': 3}, coords={'time': datetime(2022, 1, 3)})\n",
    "# ds3 = ds3.expand_dims(dim='time')\n",
    "# ds4 = xr.Dataset(data_vars={'val': 4}, coords={'time': datetime(2022, 1, 4)})\n",
    "# ds4 = ds4.expand_dims(dim='time')\n",
    "# datasets = [ds1, ds2, ds3, ds4]\n",
    "\n",
    "# import time\n",
    "# start_time = time.time()\n",
    "# print('************** Processing Zarr **************')\n",
    "\n",
    "# datasets = datasets\n",
    "\n",
    "# @dask.delayed\n",
    "# def writing_zarr(ds, overwrite):\n",
    "#     if overwrite:\n",
    "#         # to_zarr() lazily\n",
    "#         z = ds.to_zarr(store, mode='w', consolidated=True)\n",
    "#     else:\n",
    "#         z = ds.to_zarr(store, mode='a', append_dim='time', consolidated=True)\n",
    "#     return z\n",
    "\n",
    "# delayed_tasks = []\n",
    "# for i, ds in enumerate(datasets):\n",
    "#     overwrite = True if i == 0 else False\n",
    "#     delayed_tasks.append(writing_zarr(ds, overwrite))\n",
    "\n",
    "# # write the first time\n",
    "# print('=> write the first time...')\n",
    "# dask.compute(delayed_tasks[0])\n",
    "# # appending the rest in parallel\n",
    "# print('=> appending the rest in parallel...')\n",
    "# dask.compute(*delayed_tasks[1:])\n",
    "\n",
    "# print('*********************************************')\n",
    "# print(\"---------- Total: %.2f seconds ----------\" % (time.time() - start_time))\n",
    "# print('*********************************************')\n",
    "\n",
    "# temp_path = 's3://imos-data-pixeldrill/vhnguyen/emr/argo/temp/temp.zarr'\n",
    "# data = xr.open_zarr(temp_path)\n",
    "# # data = xr.open_zarr('s3://imos-data-pixeldrill/vhnguyen/ec2/argo/big.zarr/')\n",
    "# print(data.val.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93f8d75-4f14-492a-96cf-237656ae2de5",
   "metadata": {},
   "source": [
    "**with @dask.delayed**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb47263b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @dask.delayed\n",
    "# def process_float(s3_uri):\n",
    "#     preproc = partial(process_mf,levels=3000)\n",
    "#     file = read_dataset_inmemory(s3_uri)\n",
    "#     data = preproc(file)\n",
    "#     return data\n",
    "\n",
    "\n",
    "# start_time = time.time()\n",
    "# print('************** Processing Zarr **************')\n",
    "# for i in tqdm(range(len(all_chunked_paths))):\n",
    "#     overwrite = True if i == 0 else False\n",
    "#     if i >= start_from:\n",
    "#         datasets = []\n",
    "#         for path in all_chunked_paths[i]:\n",
    "#             # datasets.append(process_float(path))\n",
    "#             temp = process_float(path)\n",
    "#             datasets.append(temp.persist())\n",
    "#         zarrs = dask.compute(*datasets)   \n",
    "#         zarrs_remote = client.scatter(zarrs) #send zarrs to cluster\n",
    "#         ds_future = [client.submit(xr.concat, zarrs_remote, \n",
    "#                                    dim='N_PROF', coords='minimal',\n",
    "#                                    compat='override',combine_attrs='override', \n",
    "#                                    fill_value='')]\n",
    "#         for done in as_completed(ds_future):\n",
    "#             ds = done.result()\n",
    "#             for var in ds.data_vars:\n",
    "#                 ds[var].encoding = {} \n",
    "#             if overwrite:\n",
    "#                 # to_zarr() lazily\n",
    "#                 z = ds.to_zarr(store, mode='w', consolidated=True, compute=False)\n",
    "#             else:\n",
    "#                 z = ds.to_zarr(store, mode='a', append_dim='N_PROF', consolidated=True, compute=False)\n",
    "#             z_persisted = z.persist()\n",
    "#             z_persisted.compute()\n",
    "#         else:\n",
    "#             pass\n",
    "\n",
    "# print('*********************************************')\n",
    "# print(\"---------- Total: %.2f seconds ----------\" % (time.time() - start_time))\n",
    "# print('*********************************************')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba128cf-66ed-45ec-8a66-b96ea1c103c8",
   "metadata": {},
   "source": [
    "**Dask future**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a50a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_float(s3_uri):\n",
    "#     preproc = partial(process_mf,levels=3000)\n",
    "#     file = read_dataset_inmemory(s3_uri)\n",
    "#     data = preproc(file)\n",
    "#     return data\n",
    "\n",
    "# start_time = time.time()\n",
    "# print('************** Processing Zarr **************')\n",
    "# for i in tqdm(range(len(all_chunked_paths))):\n",
    "#     overwrite = True if i == 0 else False\n",
    "#     if i >= start_from:\n",
    "#         futures = []\n",
    "#         for path in all_chunked_paths[i]:\n",
    "#             futures.append(client.submit(process_float, path, retries=10))\n",
    "\n",
    "#         zarrs = client.gather(futures) # result return to local from the cluster\n",
    "#         ds = xr.concat(zarrs, dim='N_PROF', coords='minimal',compat='override',combine_attrs='override', fill_value='')\n",
    "#         # chunked = ds.chunk(chunks=1000)\n",
    "#         for var in ds.data_vars:\n",
    "#             ds[var].encoding = {}\n",
    "\n",
    "#         if overwrite:\n",
    "#             z = ds.to_zarr(store, mode='w', consolidated=True, compute=False) # return delayed obj\n",
    "#         else:\n",
    "#             z = ds.to_zarr(store, mode='a', append_dim='N_PROF', consolidated=True, compute=False)\n",
    "\n",
    "#         z_persisted = z.persist()\n",
    "#         z_persisted.compute()\n",
    "#     else:\n",
    "#         pass\n",
    "    \n",
    "# print('*********************************************')\n",
    "# print(\"---------- Total: %.2f seconds ----------\" % (time.time() - start_time))\n",
    "# print('*********************************************')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026fcbdf",
   "metadata": {},
   "source": [
    "### Open Zarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e31c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "temp_path = 's3://imos-data-pixeldrill/vhnguyen/test/temp/temp.zarr'\n",
    "data = xr.open_zarr(temp_path)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41f08bb-d1a8-45a5-8401-531ed467b553",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "def np_dt64_to_dt(in_datetime: np.datetime64) -> str:\n",
    "    \"\"\"Convert numpy datetime64 to datetime\"\"\"\n",
    "    dt = datetime.fromtimestamp(in_datetime.astype(int) / 1e9)\n",
    "    return dt.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "nprof = 0 #Selected profile\n",
    "plt.scatter(data.PSAL_ADJUSTED[nprof], data.TEMP_ADJUSTED[nprof], c=data.PRES_ADJUSTED[nprof], cmap='viridis_r')\n",
    "plt.xlabel('Salinity');\n",
    "plt.ylabel('Temperature (°C)')\n",
    "\n",
    "cbh = plt.colorbar();\n",
    "cbh.set_label('Pressure (dbar)')\n",
    "\n",
    "plt.grid()\n",
    "plt.title('Argo Float #%d on %s' % (data.PLATFORM_NUMBER[nprof].values, np_dt64_to_dt(data.JULD[nprof].values)), fontweight='bold');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d28c319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster.close()\n",
    "# client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a986787",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dask)",
   "language": "python",
   "name": "dask"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
