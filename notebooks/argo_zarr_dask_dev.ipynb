{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0a9dc46",
   "metadata": {},
   "source": [
    "### Python imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60dcdb94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from time import sleep\n",
    "from functools import partial\n",
    "import time\n",
    "import numcodecs\n",
    "import zarr\n",
    "import dask\n",
    "from dask.distributed import as_completed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ded6fd",
   "metadata": {},
   "source": [
    "### Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a97417b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import boto3\n",
    "\n",
    "def read_dataset_inmemory(s3_path: str) -> xr.Dataset:\n",
    "    \"\"\"Read a NetCDF as an XArray using in-memory data\"\"\"\n",
    "    try:\n",
    "        with io.BytesIO() as inmemoryfile:\n",
    "            # Use boto to download a file to memory\n",
    "            s3 = boto3.client(\"s3\")\n",
    "            bucket, key = s3_path.replace(\"s3://\", \"\").split(\"/\", 1)\n",
    "            s3.download_fileobj(bucket, key, inmemoryfile)\n",
    "            inmemoryfile.seek(0)\n",
    "\n",
    "            return xr.open_dataset(inmemoryfile)\n",
    "    except ValueError as e:\n",
    "        print(f\"Failed to open the file with error: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386a4b5f",
   "metadata": {},
   "source": [
    "### Argo Processor\n",
    "\n",
    "**Source notebook**\n",
    "https://medium.com/@nicolasmortimer/argo-floats-zarr-and-pangeo-d74fc6d4ce35\n",
    "\n",
    "*Written by Nicolas Mortimer*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61b652f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_types ={'CONFIG_MISSION_NUMBER':'float32','CYCLE_NUMBER':'float32','DATA_CENTRE':'|U2','DATA_MODE':'|U1',\n",
    "             'DATA_STATE_INDICATOR':'|U4','DC_REFERENCE':'|U32','DIRECTION':'|U1','FIRMWARE_VERSION':'|U32',\n",
    "             'FLOAT_SERIAL_NO':'|U32','JULD':'float32','JULD_LOCATION':'float32','JULD_QC':'|U1','LATITUDE':'float32',\n",
    "             'LONGITUDE':'float32','PI_NAME':'|U64','PLATFORM_NUMBER':'|U8','PLATFORM_TYPE':'|U32','POSITIONING_SYSTEM':'|U8',\n",
    "             'POSITION_QC':'|U1','PRES':'float32','PRES_ADJUSTED':'float32','PRES_ADJUSTED_ERROR':'float32',\n",
    "             'PRES_ADJUSTED_QC':'|U1','PRES_QC':'|U1','PROFILE_PRES_QC':'|U1','PROFILE_PSAL_QC':'|U1','PROFILE_TEMP_QC':'|U1',\n",
    "             'PROJECT_NAME':'|U64','PSAL':'float32','PSAL_ADJUSTED':'float32','PSAL_ADJUSTED_ERROR':'float32',\n",
    "             'PSAL_ADJUSTED_QC':'|U1','PSAL_QC':'|U1','TEMP':'float32','TEMP_ADJUSTED':'float32','TEMP_ADJUSTED_ERROR':'float32',\n",
    "             'TEMP_ADJUSTED_QC':'|U1','TEMP_QC':'|U1','VERTICAL_SAMPLING_SCHEME':'|U256','WMO_INST_TYPE':'|U4'}\n",
    "\n",
    "data_levels =['PRES','PRES_ADJUSTED','PRES_ADJUSTED_ERROR','PRES_ADJUSTED_QC','PRES_QC','PSAL','PSAL_ADJUSTED',\n",
    "              'PSAL_ADJUSTED_ERROR','PSAL_ADJUSTED_QC','PSAL_QC','TEMP','TEMP_ADJUSTED','TEMP_ADJUSTED_ERROR',\n",
    "              'TEMP_ADJUSTED_QC','TEMP_QC']\n",
    "\n",
    "def process_mf(dsinput,levels,data_types=data_types,data_levels=data_levels):\n",
    "    ds = xr.Dataset()\n",
    "    dims =('N_PROF','N_LEVELS')\n",
    "    # The number of profiles is indicated by the N_PROF dimension\n",
    "    # The number of pressure levels is indicated by the N_LEVELS dimension\n",
    "    pading =xr.DataArray(np.ones((len(dsinput.N_PROF),levels-len( dsinput.N_LEVELS))) *np.nan,dims=dims)\n",
    "    pad_qc = xr.DataArray(np.chararray((len(dsinput.N_PROF),levels-len( dsinput.N_LEVELS))),dims=dims)\n",
    "    pad_qc[:] = b' '\n",
    "    for varname in data_types.keys():\n",
    "        if varname in dsinput.data_vars:\n",
    "            da = dsinput[varname]\n",
    "            if 'N_LEVELS' in da.dims:   \n",
    "                if varname in dsinput.data_vars:\n",
    "                    if varname.endswith('QC'):\n",
    "                        da = xr.concat([dsinput[varname],pad_qc],dim='N_LEVELS').astype(data_types[varname])\n",
    "                    else:\n",
    "                        da = xr.concat([dsinput[varname],pading],dim='N_LEVELS').astype(data_types[varname])\n",
    "            else:\n",
    "                da = dsinput[varname].astype(data_types[varname])\n",
    "        else:\n",
    "            if varname in data_levels:\n",
    "                if data_types[varname]=='float32':\n",
    "                    da = xr.DataArray(np.ones((len(dsinput.N_PROF),levels), dtype='float32')*np.nan , name=varname, dims=['N_PROF','N_LEVELS'])\n",
    "                else:\n",
    "                    p=np.chararray((len(dsinput.N_PROF),levels))\n",
    "                    p[:]=b'0'\n",
    "                    da = xr.DataArray(p.astype(data_types[varname]), name=varname, dims=['N_PROF','N_LEVELS'])\n",
    "            else:\n",
    "                if data_types[varname]=='float32':\n",
    "                    da = xr.DataArray(np.ones(len(dsinput.N_PROF), dtype=\"float32\")*np.nan , name=varname, dims=['N_PROF'])\n",
    "                else:\n",
    "                    p=np.chararray((len(dsinput.N_PROF)))\n",
    "                    p[:]=b'0'\n",
    "                    da = xr.DataArray(p.astype(data_types[varname]), name=varname, dims=['N_PROF'])\n",
    "        if not ('HISTORY' in varname) and ('N_CALIB' not in da.dims) and ('N_PARAM' not in da.dims) and  ('N_PROF' in da.dims):\n",
    "                ds[varname]= da\n",
    "    return ds.chunk({'N_LEVELS':levels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d778388-00ed-421d-8dc3-fcb7faf2299e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def process_float(s3_uri):\n",
    "    preproc = partial(process_mf,levels=3000)\n",
    "    file = read_dataset_inmemory(s3_uri)\n",
    "    data = preproc(file)\n",
    "    return data\n",
    "\n",
    "@dask.delayed\n",
    "def export_zarr(store, ds, overwrite):\n",
    "    if overwrite:\n",
    "        z = ds.to_zarr(store, mode='w', consolidated=True)\n",
    "    else:\n",
    "        z = ds.to_zarr(store, mode='a', append_dim='N_PROF', consolidated=True)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7973e1-f828-4689-9e09-bd67971198d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(lst, n):\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beefc67e",
   "metadata": {},
   "source": [
    "### Generate dataset and export to Zarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e8038b-6f89-49a0-8838-ecc98ff2ca7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import s3fs\n",
    "s3 = s3fs.S3FileSystem(anon=False)\n",
    "\n",
    "input_paths = []\n",
    "# glob_result = s3.glob('s3://imos-data/IMOS/Argo/dac/csiro/7900324/profiles/*.nc')\n",
    "glob_result = s3.glob('s3://imos-data/IMOS/Argo/dac/csiro/7900324/profiles/*.nc')\n",
    "input_paths.extend(['s3://' + path for path in glob_result])\n",
    "store_path = 's3://imos-data-pixeldrill/vhnguyen/emr/argo/temp/temp.zarr'\n",
    "# store_path = './emr/argo.zarr/'\n",
    "# emr_dns = 'localhost'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d29a44-e750-497f-b165-b6f781d55a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_paths[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3449e170-ccff-42c0-a09f-5a83098c250d",
   "metadata": {},
   "source": [
    "### Using Dask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07cd8b8-9e19-410a-be28-0fed24e53a54",
   "metadata": {},
   "source": [
    "#### Fargate cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a34bb4-b98a-4d07-afb1-eda1e85644d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dask_cloudprovider.aws import FargateCluster\n",
    "# cluster = FargateCluster(image=\"ghcr.io/vietnguyengit/vietnguyen-dask:main\", scheduler_timeout=\"60 minutes\", task_role_arn=\"arn:aws:iam::615645230945:role/ManualDaskZarrCreation\",\n",
    "#                          scheduler_cpu=4096, scheduler_mem=30720, n_workers=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51862bf-9baa-41dd-aec5-aec2b8dfeea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dask.distributed import Client\n",
    "# client = Client(cluster)\n",
    "# display(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a41eb3-a1a1-4b72-b2d2-95d30e345b5f",
   "metadata": {},
   "source": [
    "#### Local cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf41333-3ad5-4802-b47a-a7aa3c420dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dask\n",
    "# from dask.distributed import Client, LocalCluster\n",
    "# with dask.config.set({'temporary_directory': '/home/vietnguyen/dask/'}):\n",
    "#     # set up cluster and workers\n",
    "#     cluster = LocalCluster(n_workers=4, memory_limit='8GB', processes=True, \n",
    "#                            threads_per_worker=4, dashboard_address=':34273', ip=emr_dns)\n",
    "#     client = Client(address=cluster.scheduler_address)\n",
    "\n",
    "# print(f'http://{emr_dns}'+':{port}/status'.format(port=client.scheduler_info().get('services').get('dashboard')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e90580-f1e8-468b-b8e6-3de86fdf2372",
   "metadata": {},
   "source": [
    "#### Dask extension local cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab953a4-9768-4d08-ad1a-ffd4853f44fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dask.distributed import Client\n",
    "\n",
    "# client = Client(\"tcp://127.0.0.1:34933\")\n",
    "# client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca3ae46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('************** Initilising Futures **************')\n",
    "print('************** Processing Zarr **************\\n')\n",
    "\n",
    "store= s3fs.S3Map(root=f'{store_path}', s3=s3, check=False)\n",
    "start_time = time.time()\n",
    "\n",
    "chunked_paths = list(chunks(input_paths, 2))\n",
    "delayed_export_zarrs = []\n",
    "for i, chunked_path in enumerate(chunked_paths):\n",
    "    datasets = []\n",
    "    for path in chunked_path:\n",
    "        datasets.append(process_float(path))\n",
    "    zarrs = dask.compute(*datasets)\n",
    "    zarrs_remote = client.scatter(zarrs) #send zarrs to cluster\n",
    "    ds = client.submit(xr.concat, zarrs_remote, dim='N_PROF', coords='minimal',compat='override',combine_attrs='override', fill_value='')\n",
    "    overwrite = True if i == 0 else False\n",
    "    delayed_export_zarrs.append(client.submit(export_zarr, store, ds, overwrite))\n",
    "    \n",
    "for done in as_completed(delayed_export_zarrs):\n",
    "    dask.compute(done.result())\n",
    "\n",
    "# futures = []\n",
    "# for i in tqdm(range(len(input_paths))):\n",
    "#     futures.append(client.submit(process_float, input_paths[i], retries=20))\n",
    "        \n",
    "print('*********************************************')\n",
    "print(\"---------- Total: %.2f seconds ----------\" % (time.time() - start_time))\n",
    "print('*********************************************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe49e28-b16d-407c-8cb1-30ae2bd55cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Break the futures list into smaller chunks\n",
    "# chunked_futures = list(chunks(futures, 1))\n",
    "\n",
    "# print('************** Processing Zarr **************')\n",
    "# start_time = time.time()\n",
    "# for i, chunked_future in enumerate(chunked_futures):\n",
    "#     zarrs = client.gather(chunked_future) # result return to local from the cluster\n",
    "#     ds = xr.concat(zarrs, dim='N_PROF', coords='minimal',compat='override',combine_attrs='override', fill_value='')\n",
    "#     # chunked = ds.chunk(chunks=1000)\n",
    "#     for var in ds.data_vars:\n",
    "#         ds[var].encoding = {}\n",
    "#     if i == 0:\n",
    "#         # to_zarr() lazily\n",
    "#         z = ds.to_zarr(store, mode='w', consolidated=True, compute=False)\n",
    "#     else:\n",
    "#         z = ds.to_zarr(store, mode='a', append_dim='N_PROF', consolidated=True, compute=False)\n",
    "#     z.compute()\n",
    "# print('*********************************************')\n",
    "# print(\"---------- Total: %.2f seconds ----------\" % (time.time() - start_time))\n",
    "# print('*********************************************')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57facc6e",
   "metadata": {},
   "source": [
    "### Open Zarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b6bdd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# data = xr.open_zarr(store_path)\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e21cc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# import matplotlib.pyplot as plt\n",
    "# from datetime import datetime\n",
    "# nprof = 200\n",
    "# def np_dt64_to_dt(in_datetime: np.datetime64) -> str:\n",
    "#     \"\"\"Convert numpy datetime64 to datetime\"\"\"\n",
    "#     dt = datetime.fromtimestamp(in_datetime.astype(int) / 1e9)\n",
    "#     return dt.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "\n",
    "# plt.figure(figsize=(8,6))\n",
    "\n",
    "# nprof = round(len(input_paths) / 2) #Selected profile\n",
    "# plt.scatter(data.PSAL_ADJUSTED[nprof], data.TEMP_ADJUSTED[nprof], c=data.PRES_ADJUSTED[nprof], cmap='viridis_r')\n",
    "# plt.xlabel('Salinity');\n",
    "# plt.ylabel('Temperature (Â°C)')\n",
    "\n",
    "# cbh = plt.colorbar();\n",
    "# cbh.set_label('Pressure (dbar)')\n",
    "\n",
    "# plt.grid()\n",
    "# plt.title('Argo Float #%d on %s' % (data.PLATFORM_NUMBER[nprof].values, np_dt64_to_dt(data.JULD[nprof].values)), fontweight='bold');\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcf1dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster.close()\n",
    "# client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a73ee6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dask)",
   "language": "python",
   "name": "dask"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
